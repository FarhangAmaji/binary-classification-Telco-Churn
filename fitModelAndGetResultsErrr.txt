fitModelAndGetResults Errrrrrr noCrossVal LinearDiscriminantAnalysis {'solver': 'lsqr', 'shrinkage': 'auto', 'priors': None, 'n_components': 2, 'tol': 0.0001} time: 1684765229.5547729 
errrr: n_components cannot be larger than min(n_features, n_classes - 1). 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\discriminant_analysis.py", line 608, in fit
    raise ValueError(
ValueError: n_components cannot be larger than min(n_features, n_classes - 1).
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearDiscriminantAnalysis {'solver': 'lsqr', 'shrinkage': 'auto', 'priors': None, 'n_components': 2, 'tol': 1e-05} time: 1684765229.5667605 
errrr: n_components cannot be larger than min(n_features, n_classes - 1). 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\discriminant_analysis.py", line 608, in fit
    raise ValueError(
ValueError: n_components cannot be larger than min(n_features, n_classes - 1).
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearDiscriminantAnalysis {'solver': 'lsqr', 'shrinkage': 'auto', 'priors': [0.1, 0.9], 'n_components': 2, 'tol': 0.0001} time: 1684765229.57676 
errrr: n_components cannot be larger than min(n_features, n_classes - 1). 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\discriminant_analysis.py", line 608, in fit
    raise ValueError(
ValueError: n_components cannot be larger than min(n_features, n_classes - 1).
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearDiscriminantAnalysis {'solver': 'lsqr', 'shrinkage': 'auto', 'priors': [0.1, 0.9], 'n_components': 2, 'tol': 1e-05} time: 1684765229.5867586 
errrr: n_components cannot be larger than min(n_features, n_classes - 1). 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\discriminant_analysis.py", line 608, in fit
    raise ValueError(
ValueError: n_components cannot be larger than min(n_features, n_classes - 1).
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearDiscriminantAnalysis {'solver': 'lsqr', 'shrinkage': 'auto', 'priors': [0.3, 0.7], 'n_components': 2, 'tol': 0.0001} time: 1684765229.5967593 
errrr: n_components cannot be larger than min(n_features, n_classes - 1). 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\discriminant_analysis.py", line 608, in fit
    raise ValueError(
ValueError: n_components cannot be larger than min(n_features, n_classes - 1).
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearDiscriminantAnalysis {'solver': 'lsqr', 'shrinkage': 'auto', 'priors': [0.3, 0.7], 'n_components': 2, 'tol': 1e-05} time: 1684765229.6057725 
errrr: n_components cannot be larger than min(n_features, n_classes - 1). 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\discriminant_analysis.py", line 608, in fit
    raise ValueError(
ValueError: n_components cannot be larger than min(n_features, n_classes - 1).
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearDiscriminantAnalysis {'solver': 'eigen', 'shrinkage': 'auto', 'priors': None, 'n_components': 2, 'tol': 0.0001} time: 1684765229.614759 
errrr: n_components cannot be larger than min(n_features, n_classes - 1). 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\discriminant_analysis.py", line 608, in fit
    raise ValueError(
ValueError: n_components cannot be larger than min(n_features, n_classes - 1).
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearDiscriminantAnalysis {'solver': 'eigen', 'shrinkage': 'auto', 'priors': None, 'n_components': 2, 'tol': 1e-05} time: 1684765229.624773 
errrr: n_components cannot be larger than min(n_features, n_classes - 1). 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\discriminant_analysis.py", line 608, in fit
    raise ValueError(
ValueError: n_components cannot be larger than min(n_features, n_classes - 1).
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearDiscriminantAnalysis {'solver': 'eigen', 'shrinkage': 'auto', 'priors': [0.1, 0.9], 'n_components': 2, 'tol': 0.0001} time: 1684765229.6337726 
errrr: n_components cannot be larger than min(n_features, n_classes - 1). 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\discriminant_analysis.py", line 608, in fit
    raise ValueError(
ValueError: n_components cannot be larger than min(n_features, n_classes - 1).
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearDiscriminantAnalysis {'solver': 'eigen', 'shrinkage': 'auto', 'priors': [0.1, 0.9], 'n_components': 2, 'tol': 1e-05} time: 1684765229.6427722 
errrr: n_components cannot be larger than min(n_features, n_classes - 1). 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\discriminant_analysis.py", line 608, in fit
    raise ValueError(
ValueError: n_components cannot be larger than min(n_features, n_classes - 1).
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearDiscriminantAnalysis {'solver': 'eigen', 'shrinkage': 'auto', 'priors': [0.3, 0.7], 'n_components': 2, 'tol': 0.0001} time: 1684765229.6527584 
errrr: n_components cannot be larger than min(n_features, n_classes - 1). 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\discriminant_analysis.py", line 608, in fit
    raise ValueError(
ValueError: n_components cannot be larger than min(n_features, n_classes - 1).
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearDiscriminantAnalysis {'solver': 'eigen', 'shrinkage': 'auto', 'priors': [0.3, 0.7], 'n_components': 2, 'tol': 1e-05} time: 1684765229.6617725 
errrr: n_components cannot be larger than min(n_features, n_classes - 1). 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\discriminant_analysis.py", line 608, in fit
    raise ValueError(
ValueError: n_components cannot be larger than min(n_features, n_classes - 1).
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 1, 'class_weight': None, 'max_iter': 1000} time: 1684765229.793772 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 1, 'class_weight': None, 'max_iter': 2000} time: 1684765229.805772 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 1, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765229.8167589 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 1, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765229.8277717 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 2, 'class_weight': None, 'max_iter': 1000} time: 1684765229.8387606 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 2, 'class_weight': None, 'max_iter': 2000} time: 1684765229.8497581 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 2, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765229.8607588 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 2, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765229.8717587 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 5, 'class_weight': None, 'max_iter': 1000} time: 1684765229.8827605 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 5, 'class_weight': None, 'max_iter': 2000} time: 1684765229.8937604 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 5, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765229.9047737 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 5, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765229.9167597 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 1, 'class_weight': None, 'max_iter': 1000} time: 1684765229.9277608 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 1, 'class_weight': None, 'max_iter': 2000} time: 1684765229.9377718 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 1, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765229.9487593 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 1, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765229.9607596 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 2, 'class_weight': None, 'max_iter': 1000} time: 1684765229.9737618 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 2, 'class_weight': None, 'max_iter': 2000} time: 1684765229.9877603 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 2, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765230.0017595 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 2, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765230.012759 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 5, 'class_weight': None, 'max_iter': 1000} time: 1684765230.0247598 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 5, 'class_weight': None, 'max_iter': 2000} time: 1684765230.0367584 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 5, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765230.048762 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 5, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765230.0617597 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 1, 'class_weight': None, 'max_iter': 1000} time: 1684765230.0737584 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 1, 'class_weight': None, 'max_iter': 2000} time: 1684765230.0857592 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 1, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765230.0987594 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 1, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765230.11076 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 2, 'class_weight': None, 'max_iter': 1000} time: 1684765230.12076 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 2, 'class_weight': None, 'max_iter': 2000} time: 1684765230.131759 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 2, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765230.1427572 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 2, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765230.1527572 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 5, 'class_weight': None, 'max_iter': 1000} time: 1684765230.1637585 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 5, 'class_weight': None, 'max_iter': 2000} time: 1684765230.1747725 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 5, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765230.1857722 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 5, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765230.1967702 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 1, 'class_weight': None, 'max_iter': 1000} time: 1684765230.207772 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 1, 'class_weight': None, 'max_iter': 2000} time: 1684765230.2187722 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 1, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765230.229861 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 1, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765230.240848 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 2, 'class_weight': None, 'max_iter': 1000} time: 1684765230.2518487 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 2, 'class_weight': None, 'max_iter': 2000} time: 1684765230.263861 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 2, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765230.2828484 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 2, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765230.294849 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 5, 'class_weight': None, 'max_iter': 1000} time: 1684765230.3070705 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 5, 'class_weight': None, 'max_iter': 2000} time: 1684765230.31785 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 5, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765230.330063 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 5, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765230.3418484 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 1, 'class_weight': None, 'max_iter': 1000} time: 1684765230.3540676 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 1, 'class_weight': None, 'max_iter': 2000} time: 1684765230.3648484 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 1, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765230.376848 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 1, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765230.3888483 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 2, 'class_weight': None, 'max_iter': 1000} time: 1684765230.3998516 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 2, 'class_weight': None, 'max_iter': 2000} time: 1684765230.4120777 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 2, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765230.423062 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 2, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765230.434849 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 5, 'class_weight': None, 'max_iter': 1000} time: 1684765230.4458497 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 5, 'class_weight': None, 'max_iter': 2000} time: 1684765230.456848 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 5, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765230.4679103 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 5, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765230.4789102 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 1, 'class_weight': None, 'max_iter': 1000} time: 1684765230.488909 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 1, 'class_weight': None, 'max_iter': 2000} time: 1684765230.4998972 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 1, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765230.510909 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 1, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765230.5209649 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 2, 'class_weight': None, 'max_iter': 1000} time: 1684765230.5309656 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 2, 'class_weight': None, 'max_iter': 2000} time: 1684765230.5419667 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 2, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765230.5529628 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 2, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765230.5639627 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 5, 'class_weight': None, 'max_iter': 1000} time: 1684765230.574963 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 5, 'class_weight': None, 'max_iter': 2000} time: 1684765230.5849655 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 5, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765230.5959656 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 0.0001, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 5, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765230.6069632 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 1, 'class_weight': None, 'max_iter': 1000} time: 1684765230.6169658 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 1, 'class_weight': None, 'max_iter': 2000} time: 1684765230.6279666 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 1, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765230.6389668 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 1, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765230.6509697 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 2, 'class_weight': None, 'max_iter': 1000} time: 1684765230.6609693 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 2, 'class_weight': None, 'max_iter': 2000} time: 1684765230.6708822 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 2, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765230.6830685 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 2, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765230.6938908 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 5, 'class_weight': None, 'max_iter': 1000} time: 1684765230.7050538 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 5, 'class_weight': None, 'max_iter': 2000} time: 1684765230.7168028 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 5, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765230.7278025 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 5, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765230.7398012 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 1, 'class_weight': None, 'max_iter': 1000} time: 1684765230.7538047 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 1, 'class_weight': None, 'max_iter': 2000} time: 1684765230.7648017 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 1, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765230.7778025 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 1, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765230.7888043 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 2, 'class_weight': None, 'max_iter': 1000} time: 1684765230.799801 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 2, 'class_weight': None, 'max_iter': 2000} time: 1684765230.8118026 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 2, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765230.8238032 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 2, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765230.835801 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 5, 'class_weight': None, 'max_iter': 1000} time: 1684765230.847803 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 5, 'class_weight': None, 'max_iter': 2000} time: 1684765230.8578172 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 5, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765230.8688028 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 1.0, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 5, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765230.8798022 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 1, 'class_weight': None, 'max_iter': 1000} time: 1684765230.8898158 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 1, 'class_weight': None, 'max_iter': 2000} time: 1684765230.900806 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 1, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765230.912805 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 1, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765230.9233954 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 2, 'class_weight': None, 'max_iter': 1000} time: 1684765230.9343953 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 2, 'class_weight': None, 'max_iter': 2000} time: 1684765230.9443955 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 2, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765230.9563947 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 2, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765230.968396 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 5, 'class_weight': None, 'max_iter': 1000} time: 1684765230.980397 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 5, 'class_weight': None, 'max_iter': 2000} time: 1684765230.9923975 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 5, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765231.0033982 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 5, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765231.0153992 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 1, 'class_weight': None, 'max_iter': 1000} time: 1684765231.0263984 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 1, 'class_weight': None, 'max_iter': 2000} time: 1684765231.038398 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 1, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765231.0493975 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 1, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765231.0603983 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 2, 'class_weight': None, 'max_iter': 1000} time: 1684765231.0724149 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 2, 'class_weight': None, 'max_iter': 2000} time: 1684765231.0834157 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 2, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765231.0954127 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 2, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765231.1074107 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 5, 'class_weight': None, 'max_iter': 1000} time: 1684765231.1184106 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 5, 'class_weight': None, 'max_iter': 2000} time: 1684765231.1294105 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 5, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765231.1404104 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.1, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 5, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765231.1514106 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 1, 'class_weight': None, 'max_iter': 1000} time: 1684765231.163425 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 1, 'class_weight': None, 'max_iter': 2000} time: 1684765231.1744444 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 1, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765231.1854453 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 1, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765231.1964447 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 2, 'class_weight': None, 'max_iter': 1000} time: 1684765231.2074447 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 2, 'class_weight': None, 'max_iter': 2000} time: 1684765231.218443 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 2, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765231.2294443 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 2, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765231.2414446 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 5, 'class_weight': None, 'max_iter': 1000} time: 1684765231.252433 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 5, 'class_weight': None, 'max_iter': 2000} time: 1684765231.2644327 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 5, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765231.277449 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': True, 'intercept_scaling': 5, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765231.2894468 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 1, 'class_weight': None, 'max_iter': 1000} time: 1684765231.301447 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 1, 'class_weight': None, 'max_iter': 2000} time: 1684765231.3124483 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 1, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765231.3244467 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 1, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765231.3364482 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 2, 'class_weight': None, 'max_iter': 1000} time: 1684765231.3484514 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 2, 'class_weight': None, 'max_iter': 2000} time: 1684765231.359449 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 2, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765231.3714647 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 2, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765231.3834648 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 5, 'class_weight': None, 'max_iter': 1000} time: 1684765231.3944645 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 5, 'class_weight': None, 'max_iter': 2000} time: 1684765231.4064646 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 5, 'class_weight': 'balanced', 'max_iter': 1000} time: 1684765231.417464 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal LinearSVC {'penalty': 'l2', 'loss': 'hinge', 'dual': False, 'tol': 1e-05, 'C': 0.01, 'multi_class': 'ovr', 'fit_intercept': False, 'intercept_scaling': 5, 'class_weight': 'balanced', 'max_iter': 2000} time: 1684765231.4294653 
errrr: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_classes.py", line 274, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1223, in _fit_liblinear
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\svm\_base.py", line 1062, in _get_liblinear_solver_type
    raise ValueError(
ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 30, 'p': 1, 'outlier_label': None} time: 1684765232.7936873 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 30, 'p': 2, 'outlier_label': None} time: 1684765232.825689 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 40, 'p': 1, 'outlier_label': None} time: 1684765232.8786871 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 40, 'p': 2, 'outlier_label': None} time: 1684765232.9106877 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 50, 'p': 1, 'outlier_label': None} time: 1684765232.9656906 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 50, 'p': 2, 'outlier_label': None} time: 1684765233.002688 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'uniform', 'algorithm': 'ball_tree', 'leaf_size': 30, 'p': 1, 'outlier_label': None} time: 1684765233.298691 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'uniform', 'algorithm': 'ball_tree', 'leaf_size': 30, 'p': 2, 'outlier_label': None} time: 1684765233.6277025 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'uniform', 'algorithm': 'ball_tree', 'leaf_size': 40, 'p': 1, 'outlier_label': None} time: 1684765233.9037015 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'uniform', 'algorithm': 'ball_tree', 'leaf_size': 40, 'p': 2, 'outlier_label': None} time: 1684765234.2417011 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'uniform', 'algorithm': 'ball_tree', 'leaf_size': 50, 'p': 1, 'outlier_label': None} time: 1684765234.5176883 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'uniform', 'algorithm': 'ball_tree', 'leaf_size': 50, 'p': 2, 'outlier_label': None} time: 1684765234.8473723 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'uniform', 'algorithm': 'kd_tree', 'leaf_size': 30, 'p': 1, 'outlier_label': None} time: 1684765235.2470305 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'uniform', 'algorithm': 'kd_tree', 'leaf_size': 30, 'p': 2, 'outlier_label': None} time: 1684765235.7020435 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'uniform', 'algorithm': 'kd_tree', 'leaf_size': 40, 'p': 1, 'outlier_label': None} time: 1684765236.1180298 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'uniform', 'algorithm': 'kd_tree', 'leaf_size': 40, 'p': 2, 'outlier_label': None} time: 1684765236.5490425 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'uniform', 'algorithm': 'kd_tree', 'leaf_size': 50, 'p': 1, 'outlier_label': None} time: 1684765236.8820426 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'uniform', 'algorithm': 'kd_tree', 'leaf_size': 50, 'p': 2, 'outlier_label': None} time: 1684765237.2460418 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'uniform', 'algorithm': 'brute', 'leaf_size': 30, 'p': 1, 'outlier_label': None} time: 1684765237.3020272 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'uniform', 'algorithm': 'brute', 'leaf_size': 30, 'p': 2, 'outlier_label': None} time: 1684765237.3330276 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'uniform', 'algorithm': 'brute', 'leaf_size': 40, 'p': 1, 'outlier_label': None} time: 1684765237.3830287 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'uniform', 'algorithm': 'brute', 'leaf_size': 40, 'p': 2, 'outlier_label': None} time: 1684765237.4140275 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'uniform', 'algorithm': 'brute', 'leaf_size': 50, 'p': 1, 'outlier_label': None} time: 1684765237.4670286 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'uniform', 'algorithm': 'brute', 'leaf_size': 50, 'p': 2, 'outlier_label': None} time: 1684765237.5010273 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'distance', 'algorithm': 'auto', 'leaf_size': 30, 'p': 1, 'outlier_label': None} time: 1684765237.5560272 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'distance', 'algorithm': 'auto', 'leaf_size': 30, 'p': 2, 'outlier_label': None} time: 1684765237.5890293 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'distance', 'algorithm': 'auto', 'leaf_size': 40, 'p': 1, 'outlier_label': None} time: 1684765237.6460273 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'distance', 'algorithm': 'auto', 'leaf_size': 40, 'p': 2, 'outlier_label': None} time: 1684765237.6770291 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'distance', 'algorithm': 'auto', 'leaf_size': 50, 'p': 1, 'outlier_label': None} time: 1684765237.7260275 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'distance', 'algorithm': 'auto', 'leaf_size': 50, 'p': 2, 'outlier_label': None} time: 1684765237.7610273 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'distance', 'algorithm': 'ball_tree', 'leaf_size': 30, 'p': 1, 'outlier_label': None} time: 1684765238.0580304 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'distance', 'algorithm': 'ball_tree', 'leaf_size': 30, 'p': 2, 'outlier_label': None} time: 1684765238.3840413 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'distance', 'algorithm': 'ball_tree', 'leaf_size': 40, 'p': 1, 'outlier_label': None} time: 1684765238.6470428 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'distance', 'algorithm': 'ball_tree', 'leaf_size': 40, 'p': 2, 'outlier_label': None} time: 1684765238.972043 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'distance', 'algorithm': 'ball_tree', 'leaf_size': 50, 'p': 1, 'outlier_label': None} time: 1684765239.2590294 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'distance', 'algorithm': 'ball_tree', 'leaf_size': 50, 'p': 2, 'outlier_label': None} time: 1684765239.5730426 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'distance', 'algorithm': 'kd_tree', 'leaf_size': 30, 'p': 1, 'outlier_label': None} time: 1684765239.9760282 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'distance', 'algorithm': 'kd_tree', 'leaf_size': 30, 'p': 2, 'outlier_label': None} time: 1684765240.4100428 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'distance', 'algorithm': 'kd_tree', 'leaf_size': 40, 'p': 1, 'outlier_label': None} time: 1684765240.8110423 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'distance', 'algorithm': 'kd_tree', 'leaf_size': 40, 'p': 2, 'outlier_label': None} time: 1684765241.244013 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'distance', 'algorithm': 'kd_tree', 'leaf_size': 50, 'p': 1, 'outlier_label': None} time: 1684765241.5730133 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'distance', 'algorithm': 'kd_tree', 'leaf_size': 50, 'p': 2, 'outlier_label': None} time: 1684765241.944027 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 30, 'p': 1, 'outlier_label': None} time: 1684765242.0130122 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 30, 'p': 2, 'outlier_label': None} time: 1684765242.0460143 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 40, 'p': 1, 'outlier_label': None} time: 1684765242.1190116 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 40, 'p': 2, 'outlier_label': None} time: 1684765242.150012 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 50, 'p': 1, 'outlier_label': None} time: 1684765242.208014 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  140,  145,  147,  149,  151,
        152,  154,  156,  158,  160,  162,  164,  165,  166,  167,  168,
        169,  170,  173,  174,  177,  179,  182,  186,  187,  188,  190,
        191,  194,  195,  198,  200,  203,  204,  205,  206,  208,  211,
        214,  217,  218,  220,  222,  223,  224,  226,  227,  228,  233,
        236,  241,  242,  246,  248,  251,  257,  260,  262,  264,  265,
        267,  268,  269,  270,  271,  273,  274,  276,  279,  280,  284,
        285,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
        297,  299,  300,  301,  302,  306,  308,  309,  311,  313,  314,
        315,  316,  321,  322,  324,  325,  329,  331,  332,  333,  335,
        337,  338,  339,  340,  343,  344,  345,  347,  348,  349,  350,
        351,  353,  354,  357,  358,  361,  365,  370,  371,  372,  374,
        376,  380,  381,  384,  385,  387,  388,  389,  393,  395,  396,
        398,  399,  401,  402,  403,  404,  405,  406,  408,  410,  411,
        413,  415,  417,  418,  419,  420,  421,  424,  425,  426,  431,
        432,  433,  435,  436,  438,  439,  440,  441,  443,  444,  446,
        447,  448,  454,  455,  456,  458,  459,  463,  465,  466,  469,
        470,  472,  473,  475,  476,  477,  478,  479,  482,  483,  484,
        485,  486,  487,  488,  491,  498,  500,  501,  503,  504,  507,
        508,  510,  511,  514,  517,  518,  521,  523,  525,  528,  530,
        531,  532,  540,  541,  544,  546,  548,  549,  552,  557,  558,
        559,  565,  566,  569,  571,  572,  573,  575,  580,  581,  582,
        584,  586,  589,  590,  591,  593,  594,  595,  597,  599,  600,
        603,  605,  606,  608,  610,  611,  612,  613,  614,  615,  619,
        620,  624,  625,  626,  628,  629,  633,  635,  636,  637,  643,
        644,  646,  647,  649,  650,  651,  652,  654,  655,  656,  657,
        658,  659,  660,  661,  662,  663,  665,  670,  671,  674,  675,
        677,  678,  680,  681,  682,  683,  685,  687,  688,  689,  690,
        692,  693,  695,  699,  700,  701,  703,  706,  707,  708,  710,
        711,  714,  715,  716,  717,  718,  720,  721,  723,  725,  726,
        729,  731,  733,  735,  736,  737,  738,  741,  743,  745,  748,
        749,  751,  752,  754,  757,  760,  762,  764,  768,  770,  771,
        773,  776,  778,  781,  783,  784,  785,  786,  787,  789,  790,
        794,  795,  799,  801,  803,  806,  808,  809,  814,  815,  816,
        817,  818,  819,  821,  823,  824,  825,  826,  827,  828,  831,
        832,  833,  836,  838,  839,  840,  841,  842,  844,  846,  847,
        853,  854,  855,  856,  857,  861,  862,  863,  867,  868,  871,
        873,  876,  877,  880,  881,  882,  885,  886,  887,  891,  892,
        893,  894,  895,  896,  898,  900,  901,  904,  905,  906,  907,
        908,  912,  913,  915,  916,  917,  920,  921,  922,  923,  925,
        926,  928,  930,  931,  933,  934,  935,  938,  939,  940,  943,
        949,  953,  955,  956,  957,  959,  963,  964,  965,  966,  967,
        972,  973,  975,  976,  980,  982,  983,  984,  985,  987,  989,
        990,  992,  993,  994,  996,  997,  999, 1000, 1001, 1004, 1005,
       1006, 1007, 1008, 1009, 1013, 1015, 1016, 1018, 1019, 1021, 1022,
       1023, 1024, 1025, 1026, 1028, 1029, 1035, 1041, 1043, 1044, 1045,
       1048, 1052, 1053, 1054, 1055, 1060, 1068, 1069, 1071, 1072, 1075,
       1076, 1081, 1084, 1089, 1091, 1093, 1094, 1095, 1096, 1098, 1099,
       1102, 1104, 1105, 1107, 1110, 1113, 1114, 1116, 1117, 1119, 1122,
       1124, 1125, 1131, 1133, 1134, 1135, 1137, 1139, 1140, 1142, 1143,
       1144, 1146, 1149, 1156, 1158, 1160, 1163, 1167, 1168, 1169, 1174,
       1176, 1178, 1179, 1181, 1185, 1186, 1188, 1190, 1191, 1193, 1195,
       1197, 1199, 1201, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215,
       1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1232, 1233,
       1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246, 1247, 1249, 1251,
       1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262, 1263, 1264, 1266,
       1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280, 1281, 1282, 1283,
       1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297, 1298, 1306, 1308,
       1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322, 1323, 1326, 1328,
       1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348, 1349, 1350, 1352,
       1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372, 1374, 1376, 1377,
       1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392, 1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.0, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 50, 'p': 2, 'outlier_label': None} time: 1684765242.2390115 
errrr: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   2,    5,    8,   14,   16,   17,   20,   21,   24,   26,   32,
         37,   42,   48,   52,   54,   57,   58,   59,   60,   62,   63,
         64,   68,   71,   73,   74,   75,   79,   80,   85,   89,   90,
         91,   92,   93,   94,   97,   98,   99,  102,  103,  104,  105,
        106,  107,  108,  113,  114,  117,  118,  122,  125,  127,  128,
        131,  132,  133,  134,  135,  138,  145,  147,  149,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  166,  167,  168,  169,
        170,  173,  174,  177,  179,  182,  186,  187,  188,  190,  191,
        194,  195,  198,  200,  203,  204,  205,  206,  208,  211,  214,
        217,  218,  220,  222,  223,  224,  226,  227,  228,  233,  236,
        241,  242,  246,  248,  251,  257,  260,  262,  264,  265,  267,
        268,  269,  270,  271,  273,  274,  276,  279,  280,  284,  285,
        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,
        299,  300,  301,  302,  306,  308,  309,  311,  313,  314,  315,
        316,  321,  322,  324,  325,  329,  331,  332,  333,  335,  337,
        338,  339,  340,  343,  344,  345,  347,  348,  349,  350,  351,
        353,  354,  357,  358,  361,  365,  370,  371,  372,  374,  376,
        380,  381,  384,  385,  387,  388,  389,  393,  395,  396,  398,
        399,  401,  402,  403,  404,  405,  406,  408,  410,  411,  413,
        415,  417,  418,  419,  420,  421,  424,  425,  426,  431,  432,
        433,  435,  436,  438,  439,  440,  441,  443,  444,  446,  447,
        448,  454,  455,  456,  458,  459,  463,  465,  466,  469,  470,
        472,  473,  475,  476,  477,  478,  479,  482,  483,  484,  485,
        486,  487,  488,  491,  498,  500,  501,  503,  504,  507,  508,
        510,  511,  514,  517,  518,  521,  523,  525,  528,  530,  531,
        532,  540,  541,  544,  546,  548,  549,  552,  557,  558,  559,
        565,  566,  569,  571,  572,  573,  575,  580,  581,  582,  584,
        586,  589,  590,  591,  593,  594,  595,  597,  599,  600,  603,
        605,  606,  608,  610,  611,  612,  613,  614,  615,  619,  620,
        624,  625,  626,  628,  629,  633,  635,  636,  637,  643,  644,
        646,  647,  649,  650,  651,  652,  654,  655,  656,  657,  658,
        659,  660,  661,  662,  663,  665,  670,  671,  674,  675,  677,
        678,  680,  681,  682,  683,  687,  688,  689,  690,  692,  693,
        695,  699,  700,  701,  703,  706,  707,  708,  710,  711,  714,
        715,  716,  717,  718,  720,  721,  723,  725,  726,  729,  731,
        733,  735,  736,  737,  738,  741,  743,  745,  748,  749,  751,
        752,  754,  757,  760,  762,  764,  768,  770,  771,  773,  776,
        778,  781,  783,  784,  785,  786,  787,  789,  790,  794,  795,
        799,  801,  803,  806,  808,  809,  814,  815,  816,  817,  818,
        819,  821,  823,  824,  825,  826,  827,  828,  831,  832,  833,
        836,  838,  839,  840,  841,  842,  844,  846,  847,  853,  854,
        855,  856,  857,  861,  862,  863,  867,  868,  871,  873,  876,
        877,  880,  881,  882,  886,  887,  891,  892,  893,  894,  895,
        896,  898,  900,  904,  905,  906,  907,  908,  912,  913,  915,
        916,  917,  920,  921,  922,  923,  925,  926,  928,  930,  931,
        933,  934,  935,  938,  939,  940,  943,  949,  953,  955,  956,
        957,  959,  963,  964,  965,  966,  967,  972,  973,  975,  976,
        980,  982,  983,  984,  985,  987,  989,  990,  992,  993,  994,
        996,  997,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,
       1015, 1016, 1018, 1019, 1021, 1022, 1023, 1024, 1025, 1026, 1028,
       1029, 1035, 1041, 1043, 1044, 1045, 1048, 1052, 1053, 1055, 1060,
       1068, 1069, 1071, 1072, 1075, 1076, 1081, 1084, 1089, 1091, 1093,
       1094, 1095, 1096, 1098, 1099, 1102, 1104, 1105, 1107, 1110, 1113,
       1114, 1116, 1117, 1119, 1122, 1124, 1125, 1131, 1133, 1134, 1135,
       1137, 1139, 1140, 1142, 1143, 1144, 1146, 1149, 1156, 1158, 1160,
       1163, 1167, 1168, 1169, 1174, 1176, 1178, 1179, 1181, 1185, 1186,
       1188, 1190, 1191, 1193, 1195, 1197, 1199, 1207, 1208, 1210, 1211,
       1212, 1213, 1215, 1216, 1218, 1221, 1222, 1225, 1227, 1228, 1229,
       1230, 1232, 1233, 1234, 1235, 1236, 1238, 1241, 1243, 1244, 1246,
       1247, 1249, 1251, 1252, 1254, 1255, 1256, 1259, 1260, 1261, 1262,
       1263, 1264, 1266, 1267, 1269, 1270, 1271, 1275, 1278, 1279, 1280,
       1281, 1282, 1283, 1286, 1287, 1288, 1290, 1293, 1294, 1296, 1297,
       1298, 1306, 1308, 1310, 1311, 1314, 1316, 1319, 1320, 1321, 1322,
       1323, 1326, 1328, 1335, 1338, 1340, 1341, 1344, 1345, 1346, 1348,
       1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364, 1365, 1371, 1372,
       1374, 1376, 1377, 1379, 1381, 1383, 1384, 1387, 1389, 1390, 1392,
       1400], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 30, 'p': 1, 'outlier_label': None} time: 1684765242.2920148 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 30, 'p': 2, 'outlier_label': None} time: 1684765242.3250153 
errrr: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 40, 'p': 1, 'outlier_label': None} time: 1684765242.3850133 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 40, 'p': 2, 'outlier_label': None} time: 1684765242.4190116 
errrr: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 50, 'p': 1, 'outlier_label': None} time: 1684765242.4720118 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 50, 'p': 2, 'outlier_label': None} time: 1684765242.507014 
errrr: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'uniform', 'algorithm': 'ball_tree', 'leaf_size': 30, 'p': 1, 'outlier_label': None} time: 1684765242.8230267 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'uniform', 'algorithm': 'ball_tree', 'leaf_size': 30, 'p': 2, 'outlier_label': None} time: 1684765243.1550274 
errrr: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'uniform', 'algorithm': 'ball_tree', 'leaf_size': 40, 'p': 1, 'outlier_label': None} time: 1684765243.442027 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'uniform', 'algorithm': 'ball_tree', 'leaf_size': 40, 'p': 2, 'outlier_label': None} time: 1684765243.7750275 
errrr: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'uniform', 'algorithm': 'ball_tree', 'leaf_size': 50, 'p': 1, 'outlier_label': None} time: 1684765244.0770273 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'uniform', 'algorithm': 'ball_tree', 'leaf_size': 50, 'p': 2, 'outlier_label': None} time: 1684765244.406018 
errrr: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'uniform', 'algorithm': 'kd_tree', 'leaf_size': 30, 'p': 1, 'outlier_label': None} time: 1684765244.8710284 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'uniform', 'algorithm': 'kd_tree', 'leaf_size': 30, 'p': 2, 'outlier_label': None} time: 1684765245.5240297 
errrr: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'uniform', 'algorithm': 'kd_tree', 'leaf_size': 40, 'p': 1, 'outlier_label': None} time: 1684765245.9860268 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'uniform', 'algorithm': 'kd_tree', 'leaf_size': 40, 'p': 2, 'outlier_label': None} time: 1684765246.6390314 
errrr: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'uniform', 'algorithm': 'kd_tree', 'leaf_size': 50, 'p': 1, 'outlier_label': None} time: 1684765247.0350308 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'uniform', 'algorithm': 'kd_tree', 'leaf_size': 50, 'p': 2, 'outlier_label': None} time: 1684765247.5302413 
errrr: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'uniform', 'algorithm': 'brute', 'leaf_size': 30, 'p': 1, 'outlier_label': None} time: 1684765247.6122262 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'uniform', 'algorithm': 'brute', 'leaf_size': 30, 'p': 2, 'outlier_label': None} time: 1684765247.6532266 
errrr: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'uniform', 'algorithm': 'brute', 'leaf_size': 40, 'p': 1, 'outlier_label': None} time: 1684765247.7102287 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'uniform', 'algorithm': 'brute', 'leaf_size': 40, 'p': 2, 'outlier_label': None} time: 1684765247.7442298 
errrr: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'uniform', 'algorithm': 'brute', 'leaf_size': 50, 'p': 1, 'outlier_label': None} time: 1684765247.802229 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'uniform', 'algorithm': 'brute', 'leaf_size': 50, 'p': 2, 'outlier_label': None} time: 1684765247.8362277 
errrr: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'distance', 'algorithm': 'auto', 'leaf_size': 30, 'p': 1, 'outlier_label': None} time: 1684765247.889226 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'distance', 'algorithm': 'auto', 'leaf_size': 30, 'p': 2, 'outlier_label': None} time: 1684765247.927229 
errrr: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'distance', 'algorithm': 'auto', 'leaf_size': 40, 'p': 1, 'outlier_label': None} time: 1684765247.980227 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'distance', 'algorithm': 'auto', 'leaf_size': 40, 'p': 2, 'outlier_label': None} time: 1684765248.0192263 
errrr: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'distance', 'algorithm': 'auto', 'leaf_size': 50, 'p': 1, 'outlier_label': None} time: 1684765248.0832267 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'distance', 'algorithm': 'auto', 'leaf_size': 50, 'p': 2, 'outlier_label': None} time: 1684765248.1202266 
errrr: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'distance', 'algorithm': 'ball_tree', 'leaf_size': 30, 'p': 1, 'outlier_label': None} time: 1684765248.4333613 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'distance', 'algorithm': 'ball_tree', 'leaf_size': 30, 'p': 2, 'outlier_label': None} time: 1684765248.765423 
errrr: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'distance', 'algorithm': 'ball_tree', 'leaf_size': 40, 'p': 1, 'outlier_label': None} time: 1684765249.0444257 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'distance', 'algorithm': 'ball_tree', 'leaf_size': 40, 'p': 2, 'outlier_label': None} time: 1684765249.395423 
errrr: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'distance', 'algorithm': 'ball_tree', 'leaf_size': 50, 'p': 1, 'outlier_label': None} time: 1684765249.7115257 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'distance', 'algorithm': 'ball_tree', 'leaf_size': 50, 'p': 2, 'outlier_label': None} time: 1684765250.0401635 
errrr: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'distance', 'algorithm': 'kd_tree', 'leaf_size': 30, 'p': 1, 'outlier_label': None} time: 1684765250.5093086 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'distance', 'algorithm': 'kd_tree', 'leaf_size': 30, 'p': 2, 'outlier_label': None} time: 1684765251.167888 
errrr: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'distance', 'algorithm': 'kd_tree', 'leaf_size': 40, 'p': 1, 'outlier_label': None} time: 1684765251.6178792 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'distance', 'algorithm': 'kd_tree', 'leaf_size': 40, 'p': 2, 'outlier_label': None} time: 1684765252.2588763 
errrr: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'distance', 'algorithm': 'kd_tree', 'leaf_size': 50, 'p': 1, 'outlier_label': None} time: 1684765252.6228793 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'distance', 'algorithm': 'kd_tree', 'leaf_size': 50, 'p': 2, 'outlier_label': None} time: 1684765253.1068873 
errrr: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 30, 'p': 1, 'outlier_label': None} time: 1684765253.1638722 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 30, 'p': 2, 'outlier_label': None} time: 1684765253.1978726 
errrr: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 40, 'p': 1, 'outlier_label': None} time: 1684765253.2598724 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 40, 'p': 2, 'outlier_label': None} time: 1684765253.2988758 
errrr: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 50, 'p': 1, 'outlier_label': None} time: 1684765253.369873 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   17,   20,   26,   32,   37,   42,   48,
         54,   57,   58,   59,   62,   68,   73,   75,   79,   80,   89,
         90,   91,   93,   94,   97,   99,  102,  104,  105,  106,  108,
        113,  114,  117,  122,  133,  134,  138,  145,  147,  151,  152,
        154,  156,  158,  160,  162,  164,  165,  167,  170,  173,  174,
        177,  182,  187,  191,  194,  203,  206,  208,  211,  217,  218,
        222,  223,  226,  227,  233,  236,  241,  242,  248,  251,  260,
        262,  264,  268,  269,  270,  271,  273,  274,  276,  280,  284,
        285,  287,  288,  289,  290,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  314,  315,  316,  322,  324,  325,  331,
        332,  333,  335,  338,  340,  343,  344,  345,  347,  348,  349,
        350,  351,  353,  358,  365,  370,  371,  372,  374,  376,  380,
        388,  389,  393,  396,  398,  399,  402,  403,  406,  408,  410,
        411,  415,  418,  420,  421,  424,  425,  432,  436,  438,  439,
        443,  444,  446,  447,  448,  455,  456,  459,  463,  465,  469,
        472,  473,  475,  478,  482,  483,  484,  485,  487,  488,  491,
        500,  501,  504,  508,  517,  521,  523,  525,  530,  531,  540,
        544,  546,  552,  557,  558,  559,  565,  566,  569,  571,  572,
        573,  575,  584,  586,  589,  590,  591,  594,  600,  603,  605,
        606,  610,  611,  612,  613,  614,  624,  625,  629,  633,  637,
        643,  644,  646,  647,  651,  652,  655,  656,  658,  659,  661,
        662,  663,  665,  671,  674,  675,  677,  678,  680,  681,  682,
        687,  688,  689,  690,  695,  699,  700,  701,  707,  708,  710,
        711,  714,  715,  716,  717,  720,  725,  726,  731,  735,  736,
        737,  738,  741,  748,  751,  752,  754,  757,  762,  770,  771,
        773,  781,  785,  786,  787,  789,  790,  794,  795,  801,  803,
        806,  808,  815,  816,  817,  818,  819,  824,  826,  828,  831,
        832,  836,  838,  839,  841,  842,  855,  856,  861,  862,  868,
        873,  876,  877,  880,  881,  882,  887,  891,  893,  894,  895,
        896,  898,  904,  905,  906,  908,  912,  913,  915,  916,  917,
        920,  921,  922,  923,  925,  928,  935,  939,  940,  953,  955,
        959,  963,  964,  965,  967,  973,  976,  980,  982,  983,  985,
        987,  989,  996,  997,  999, 1001, 1005, 1006, 1009, 1015, 1016,
       1018, 1023, 1024, 1025, 1029, 1041, 1044, 1045, 1048, 1052, 1055,
       1060, 1068, 1069, 1071, 1072, 1075, 1076, 1084, 1089, 1091, 1093,
       1094, 1096, 1098, 1104, 1107, 1110, 1116, 1117, 1124, 1135, 1137,
       1140, 1143, 1144, 1149, 1156, 1158, 1160, 1163, 1168, 1174, 1176,
       1178, 1179, 1181, 1185, 1186, 1188, 1191, 1193, 1195, 1197, 1199,
       1210, 1211, 1218, 1221, 1222, 1227, 1232, 1233, 1234, 1236, 1238,
       1243, 1246, 1249, 1252, 1254, 1255, 1260, 1261, 1262, 1264, 1266,
       1269, 1270, 1271, 1278, 1281, 1282, 1283, 1287, 1288, 1290, 1293,
       1296, 1297, 1298, 1308, 1311, 1314, 1319, 1320, 1321, 1328, 1340,
       1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359, 1361, 1363, 1364,
       1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 1.5, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 50, 'p': 2, 'outlier_label': None} time: 1684765253.4188735 
errrr: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   8,   14,   20,   32,   42,   54,   57,   68,   75,   79,   91,
         93,   94,  105,  113,  114,  122,  133,  152,  154,  156,  164,
        177,  248,  268,  270,  274,  276,  288,  292,  295,  297,  306,
        308,  324,  344,  345,  348,  350,  353,  365,  380,  389,  396,
        399,  406,  410,  411,  420,  424,  432,  436,  438,  439,  447,
        456,  459,  475,  484,  485,  491,  501,  525,  531,  544,  559,
        565,  566,  575,  591,  600,  603,  610,  614,  624,  629,  647,
        652,  656,  674,  678,  680,  688,  689,  690,  699,  700,  701,
        715,  726,  731,  738,  741,  752,  771,  781,  787,  790,  795,
        801,  806,  815,  818,  831,  839,  856,  862,  877,  880,  882,
        894,  896,  898,  904,  908,  913,  922,  925,  959,  964,  973,
        982,  987,  989, 1005, 1009, 1015, 1025, 1076, 1094, 1107, 1135,
       1137, 1140, 1144, 1158, 1168, 1188, 1199, 1210, 1222, 1227, 1233,
       1234, 1236, 1271, 1290, 1293, 1297, 1308, 1314, 1320, 1321, 1340,
       1350, 1358, 1359, 1363, 1371, 1374, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 30, 'p': 1, 'outlier_label': None} time: 1684765253.5138767 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 30, 'p': 2, 'outlier_label': None} time: 1684765253.5928752 
errrr: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 40, 'p': 1, 'outlier_label': None} time: 1684765253.6878746 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 40, 'p': 2, 'outlier_label': None} time: 1684765253.736875 
errrr: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 50, 'p': 1, 'outlier_label': None} time: 1684765253.8238726 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 50, 'p': 2, 'outlier_label': None} time: 1684765253.8718734 
errrr: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'uniform', 'algorithm': 'ball_tree', 'leaf_size': 30, 'p': 1, 'outlier_label': None} time: 1684765254.2238748 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'uniform', 'algorithm': 'ball_tree', 'leaf_size': 30, 'p': 2, 'outlier_label': None} time: 1684765254.5578737 
errrr: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'uniform', 'algorithm': 'ball_tree', 'leaf_size': 40, 'p': 1, 'outlier_label': None} time: 1684765254.8658757 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'uniform', 'algorithm': 'ball_tree', 'leaf_size': 40, 'p': 2, 'outlier_label': None} time: 1684765255.2018878 
errrr: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'uniform', 'algorithm': 'ball_tree', 'leaf_size': 50, 'p': 1, 'outlier_label': None} time: 1684765255.5148754 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'uniform', 'algorithm': 'ball_tree', 'leaf_size': 50, 'p': 2, 'outlier_label': None} time: 1684765255.8388734 
errrr: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'uniform', 'algorithm': 'kd_tree', 'leaf_size': 30, 'p': 1, 'outlier_label': None} time: 1684765256.3798773 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'uniform', 'algorithm': 'kd_tree', 'leaf_size': 30, 'p': 2, 'outlier_label': None} time: 1684765257.0878773 
errrr: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'uniform', 'algorithm': 'kd_tree', 'leaf_size': 40, 'p': 1, 'outlier_label': None} time: 1684765257.644877 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'uniform', 'algorithm': 'kd_tree', 'leaf_size': 40, 'p': 2, 'outlier_label': None} time: 1684765258.3628876 
errrr: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'uniform', 'algorithm': 'kd_tree', 'leaf_size': 50, 'p': 1, 'outlier_label': None} time: 1684765258.7988784 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'uniform', 'algorithm': 'kd_tree', 'leaf_size': 50, 'p': 2, 'outlier_label': None} time: 1684765259.3148756 
errrr: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'uniform', 'algorithm': 'brute', 'leaf_size': 30, 'p': 1, 'outlier_label': None} time: 1684765259.3748727 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'uniform', 'algorithm': 'brute', 'leaf_size': 30, 'p': 2, 'outlier_label': None} time: 1684765259.4098723 
errrr: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'uniform', 'algorithm': 'brute', 'leaf_size': 40, 'p': 1, 'outlier_label': None} time: 1684765259.4758725 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'uniform', 'algorithm': 'brute', 'leaf_size': 40, 'p': 2, 'outlier_label': None} time: 1684765259.5158727 
errrr: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'uniform', 'algorithm': 'brute', 'leaf_size': 50, 'p': 1, 'outlier_label': None} time: 1684765259.5758727 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'uniform', 'algorithm': 'brute', 'leaf_size': 50, 'p': 2, 'outlier_label': None} time: 1684765259.6168725 
errrr: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'distance', 'algorithm': 'auto', 'leaf_size': 30, 'p': 1, 'outlier_label': None} time: 1684765259.6808727 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'distance', 'algorithm': 'auto', 'leaf_size': 30, 'p': 2, 'outlier_label': None} time: 1684765259.7168734 
errrr: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'distance', 'algorithm': 'auto', 'leaf_size': 40, 'p': 1, 'outlier_label': None} time: 1684765259.7698724 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'distance', 'algorithm': 'auto', 'leaf_size': 40, 'p': 2, 'outlier_label': None} time: 1684765259.8068755 
errrr: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'distance', 'algorithm': 'auto', 'leaf_size': 50, 'p': 1, 'outlier_label': None} time: 1684765259.8588724 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'distance', 'algorithm': 'auto', 'leaf_size': 50, 'p': 2, 'outlier_label': None} time: 1684765259.8958757 
errrr: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'distance', 'algorithm': 'ball_tree', 'leaf_size': 30, 'p': 1, 'outlier_label': None} time: 1684765260.2248793 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'distance', 'algorithm': 'ball_tree', 'leaf_size': 30, 'p': 2, 'outlier_label': None} time: 1684765260.564874 
errrr: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'distance', 'algorithm': 'ball_tree', 'leaf_size': 40, 'p': 1, 'outlier_label': None} time: 1684765260.8658748 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'distance', 'algorithm': 'ball_tree', 'leaf_size': 40, 'p': 2, 'outlier_label': None} time: 1684765261.205879 
errrr: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'distance', 'algorithm': 'ball_tree', 'leaf_size': 50, 'p': 1, 'outlier_label': None} time: 1684765261.5298762 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'distance', 'algorithm': 'ball_tree', 'leaf_size': 50, 'p': 2, 'outlier_label': None} time: 1684765261.892873 
errrr: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'distance', 'algorithm': 'kd_tree', 'leaf_size': 30, 'p': 1, 'outlier_label': None} time: 1684765262.4358785 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'distance', 'algorithm': 'kd_tree', 'leaf_size': 30, 'p': 2, 'outlier_label': None} time: 1684765263.1538746 
errrr: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'distance', 'algorithm': 'kd_tree', 'leaf_size': 40, 'p': 1, 'outlier_label': None} time: 1684765263.6988764 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'distance', 'algorithm': 'kd_tree', 'leaf_size': 40, 'p': 2, 'outlier_label': None} time: 1684765264.4268775 
errrr: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'distance', 'algorithm': 'kd_tree', 'leaf_size': 50, 'p': 1, 'outlier_label': None} time: 1684765264.8511727 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'distance', 'algorithm': 'kd_tree', 'leaf_size': 50, 'p': 2, 'outlier_label': None} time: 1684765265.384041 
errrr: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 30, 'p': 1, 'outlier_label': None} time: 1684765265.4440427 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 30, 'p': 2, 'outlier_label': None} time: 1684765265.4800408 
errrr: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 40, 'p': 1, 'outlier_label': None} time: 1684765265.5370405 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 40, 'p': 2, 'outlier_label': None} time: 1684765265.5790417 
errrr: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 50, 'p': 1, 'outlier_label': None} time: 1684765265.6410408 
errrr: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([   5,    8,   14,   16,   20,   26,   32,   37,   42,   48,   54,
         57,   58,   59,   62,   68,   75,   79,   80,   89,   90,   91,
         93,   94,   97,   99,  102,  105,  106,  113,  114,  117,  122,
        133,  138,  145,  147,  151,  152,  154,  156,  158,  160,  162,
        164,  165,  167,  170,  174,  177,  182,  191,  194,  203,  206,
        208,  211,  217,  222,  226,  227,  233,  236,  241,  242,  248,
        251,  260,  264,  268,  269,  270,  271,  273,  274,  276,  280,
        284,  285,  287,  288,  289,  292,  295,  296,  297,  299,  300,
        301,  302,  306,  308,  315,  316,  322,  324,  331,  332,  333,
        335,  338,  340,  343,  344,  345,  347,  348,  350,  353,  358,
        365,  371,  372,  380,  388,  389,  393,  396,  398,  399,  402,
        403,  406,  408,  410,  411,  415,  418,  420,  421,  424,  425,
        432,  436,  438,  439,  443,  444,  446,  447,  448,  455,  456,
        459,  463,  465,  469,  472,  473,  475,  478,  482,  484,  485,
        488,  491,  500,  501,  504,  508,  517,  521,  523,  525,  530,
        531,  544,  546,  552,  557,  558,  559,  565,  566,  569,  571,
        572,  573,  575,  584,  586,  589,  590,  591,  594,  600,  603,
        605,  606,  610,  611,  612,  614,  624,  625,  629,  633,  637,
        644,  646,  647,  651,  652,  655,  656,  658,  661,  662,  663,
        665,  674,  675,  677,  678,  680,  681,  682,  687,  688,  689,
        690,  695,  699,  700,  701,  707,  708,  710,  711,  714,  715,
        716,  717,  720,  725,  726,  731,  735,  736,  737,  738,  741,
        748,  751,  752,  754,  757,  762,  770,  771,  773,  781,  785,
        786,  787,  790,  794,  795,  801,  803,  806,  815,  816,  817,
        818,  819,  824,  826,  828,  831,  832,  836,  838,  839,  842,
        855,  856,  861,  862,  868,  873,  877,  880,  881,  882,  887,
        893,  894,  895,  896,  898,  904,  905,  906,  908,  912,  913,
        915,  916,  920,  921,  922,  923,  925,  928,  935,  939,  940,
        953,  955,  959,  964,  965,  967,  973,  976,  980,  982,  983,
        985,  987,  989,  996,  999, 1005, 1006, 1009, 1015, 1016, 1018,
       1023, 1024, 1025, 1029, 1041, 1044, 1048, 1052, 1055, 1060, 1068,
       1069, 1071, 1072, 1076, 1089, 1091, 1093, 1094, 1096, 1098, 1107,
       1110, 1116, 1117, 1124, 1135, 1137, 1140, 1144, 1149, 1156, 1158,
       1168, 1174, 1176, 1178, 1179, 1181, 1188, 1191, 1193, 1195, 1199,
       1210, 1218, 1222, 1227, 1233, 1234, 1236, 1238, 1243, 1246, 1254,
       1255, 1260, 1261, 1262, 1266, 1269, 1270, 1271, 1278, 1281, 1282,
       1283, 1287, 1290, 1293, 1296, 1297, 1298, 1308, 1311, 1314, 1320,
       1321, 1328, 1340, 1341, 1345, 1348, 1349, 1350, 1352, 1358, 1359,
       1361, 1363, 1364, 1365, 1371, 1374, 1377, 1379, 1390], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RadiusNeighborsClassifier {'radius': 2.0, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 50, 'p': 2, 'outlier_label': None} time: 1684765265.678041 
errrr: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset. 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 111, in fitModelAndGetResults
    predicteds = [predictedY('test', fitted_model.predict(data.xTest), data.yTest)]
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 597, in predict
    probs = self.predict_proba(X)
            ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\neighbors\_classification.py", line 656, in predict_proba
    raise ValueError(
ValueError: No neighbors found for test samples array([  14,   54,   79,  105,  177,  274,  276,  297,  306,  350,  410,
        424,  438,  491,  501,  525,  610,  680,  790,  806,  877,  973,
        982, 1005, 1144, 1168, 1234, 1359, 1374], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.
 
 
fitModelAndGetResults Errrrrrr noCrossVal RidgeClassifierCV {'alphas': [0.1, 1.0, 10.0], 'fit_intercept': True, 'scoring': 'accuracy', 'cv': 5, 'class_weight': None, 'store_cv_values': True} time: 1684765265.9480433 
errrr: cv!=None and store_cv_values=True are incompatible 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2543, in fit
    super().fit(X, target, sample_weight=sample_weight)
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2168, in fit
    raise ValueError("cv!=None and store_cv_values=True are incompatible")
ValueError: cv!=None and store_cv_values=True are incompatible
 
 
fitModelAndGetResults Errrrrrr noCrossVal RidgeClassifierCV {'alphas': [0.1, 1.0, 10.0], 'fit_intercept': True, 'scoring': 'accuracy', 'cv': 5, 'class_weight': 'balanced', 'store_cv_values': True} time: 1684765265.9600403 
errrr: cv!=None and store_cv_values=True are incompatible 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2543, in fit
    super().fit(X, target, sample_weight=sample_weight)
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2168, in fit
    raise ValueError("cv!=None and store_cv_values=True are incompatible")
ValueError: cv!=None and store_cv_values=True are incompatible
 
 
fitModelAndGetResults Errrrrrr noCrossVal RidgeClassifierCV {'alphas': [0.1, 1.0, 10.0], 'fit_intercept': True, 'scoring': 'accuracy', 'cv': 10, 'class_weight': None, 'store_cv_values': True} time: 1684765265.9710402 
errrr: cv!=None and store_cv_values=True are incompatible 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2543, in fit
    super().fit(X, target, sample_weight=sample_weight)
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2168, in fit
    raise ValueError("cv!=None and store_cv_values=True are incompatible")
ValueError: cv!=None and store_cv_values=True are incompatible
 
 
fitModelAndGetResults Errrrrrr noCrossVal RidgeClassifierCV {'alphas': [0.1, 1.0, 10.0], 'fit_intercept': True, 'scoring': 'accuracy', 'cv': 10, 'class_weight': 'balanced', 'store_cv_values': True} time: 1684765265.9830425 
errrr: cv!=None and store_cv_values=True are incompatible 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2543, in fit
    super().fit(X, target, sample_weight=sample_weight)
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2168, in fit
    raise ValueError("cv!=None and store_cv_values=True are incompatible")
ValueError: cv!=None and store_cv_values=True are incompatible
 
 
fitModelAndGetResults Errrrrrr noCrossVal RidgeClassifierCV {'alphas': [0.1, 1.0, 10.0], 'fit_intercept': True, 'scoring': 'f1_macro', 'cv': 5, 'class_weight': None, 'store_cv_values': True} time: 1684765265.9940429 
errrr: cv!=None and store_cv_values=True are incompatible 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2543, in fit
    super().fit(X, target, sample_weight=sample_weight)
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2168, in fit
    raise ValueError("cv!=None and store_cv_values=True are incompatible")
ValueError: cv!=None and store_cv_values=True are incompatible
 
 
fitModelAndGetResults Errrrrrr noCrossVal RidgeClassifierCV {'alphas': [0.1, 1.0, 10.0], 'fit_intercept': True, 'scoring': 'f1_macro', 'cv': 5, 'class_weight': 'balanced', 'store_cv_values': True} time: 1684765266.006043 
errrr: cv!=None and store_cv_values=True are incompatible 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2543, in fit
    super().fit(X, target, sample_weight=sample_weight)
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2168, in fit
    raise ValueError("cv!=None and store_cv_values=True are incompatible")
ValueError: cv!=None and store_cv_values=True are incompatible
 
 
fitModelAndGetResults Errrrrrr noCrossVal RidgeClassifierCV {'alphas': [0.1, 1.0, 10.0], 'fit_intercept': True, 'scoring': 'f1_macro', 'cv': 10, 'class_weight': None, 'store_cv_values': True} time: 1684765266.0170422 
errrr: cv!=None and store_cv_values=True are incompatible 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2543, in fit
    super().fit(X, target, sample_weight=sample_weight)
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2168, in fit
    raise ValueError("cv!=None and store_cv_values=True are incompatible")
ValueError: cv!=None and store_cv_values=True are incompatible
 
 
fitModelAndGetResults Errrrrrr noCrossVal RidgeClassifierCV {'alphas': [0.1, 1.0, 10.0], 'fit_intercept': True, 'scoring': 'f1_macro', 'cv': 10, 'class_weight': 'balanced', 'store_cv_values': True} time: 1684765266.0290546 
errrr: cv!=None and store_cv_values=True are incompatible 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2543, in fit
    super().fit(X, target, sample_weight=sample_weight)
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2168, in fit
    raise ValueError("cv!=None and store_cv_values=True are incompatible")
ValueError: cv!=None and store_cv_values=True are incompatible
 
 
fitModelAndGetResults Errrrrrr noCrossVal RidgeClassifierCV {'alphas': [0.1, 1.0, 10.0], 'fit_intercept': False, 'scoring': 'accuracy', 'cv': 5, 'class_weight': None, 'store_cv_values': True} time: 1684765266.0400548 
errrr: cv!=None and store_cv_values=True are incompatible 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2543, in fit
    super().fit(X, target, sample_weight=sample_weight)
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2168, in fit
    raise ValueError("cv!=None and store_cv_values=True are incompatible")
ValueError: cv!=None and store_cv_values=True are incompatible
 
 
fitModelAndGetResults Errrrrrr noCrossVal RidgeClassifierCV {'alphas': [0.1, 1.0, 10.0], 'fit_intercept': False, 'scoring': 'accuracy', 'cv': 5, 'class_weight': 'balanced', 'store_cv_values': True} time: 1684765266.051056 
errrr: cv!=None and store_cv_values=True are incompatible 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2543, in fit
    super().fit(X, target, sample_weight=sample_weight)
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2168, in fit
    raise ValueError("cv!=None and store_cv_values=True are incompatible")
ValueError: cv!=None and store_cv_values=True are incompatible
 
 
fitModelAndGetResults Errrrrrr noCrossVal RidgeClassifierCV {'alphas': [0.1, 1.0, 10.0], 'fit_intercept': False, 'scoring': 'accuracy', 'cv': 10, 'class_weight': None, 'store_cv_values': True} time: 1684765266.0620568 
errrr: cv!=None and store_cv_values=True are incompatible 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2543, in fit
    super().fit(X, target, sample_weight=sample_weight)
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2168, in fit
    raise ValueError("cv!=None and store_cv_values=True are incompatible")
ValueError: cv!=None and store_cv_values=True are incompatible
 
 
fitModelAndGetResults Errrrrrr noCrossVal RidgeClassifierCV {'alphas': [0.1, 1.0, 10.0], 'fit_intercept': False, 'scoring': 'accuracy', 'cv': 10, 'class_weight': 'balanced', 'store_cv_values': True} time: 1684765266.074044 
errrr: cv!=None and store_cv_values=True are incompatible 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2543, in fit
    super().fit(X, target, sample_weight=sample_weight)
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2168, in fit
    raise ValueError("cv!=None and store_cv_values=True are incompatible")
ValueError: cv!=None and store_cv_values=True are incompatible
 
 
fitModelAndGetResults Errrrrrr noCrossVal RidgeClassifierCV {'alphas': [0.1, 1.0, 10.0], 'fit_intercept': False, 'scoring': 'f1_macro', 'cv': 5, 'class_weight': None, 'store_cv_values': True} time: 1684765266.0840437 
errrr: cv!=None and store_cv_values=True are incompatible 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2543, in fit
    super().fit(X, target, sample_weight=sample_weight)
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2168, in fit
    raise ValueError("cv!=None and store_cv_values=True are incompatible")
ValueError: cv!=None and store_cv_values=True are incompatible
 
 
fitModelAndGetResults Errrrrrr noCrossVal RidgeClassifierCV {'alphas': [0.1, 1.0, 10.0], 'fit_intercept': False, 'scoring': 'f1_macro', 'cv': 5, 'class_weight': 'balanced', 'store_cv_values': True} time: 1684765266.096056 
errrr: cv!=None and store_cv_values=True are incompatible 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2543, in fit
    super().fit(X, target, sample_weight=sample_weight)
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2168, in fit
    raise ValueError("cv!=None and store_cv_values=True are incompatible")
ValueError: cv!=None and store_cv_values=True are incompatible
 
 
fitModelAndGetResults Errrrrrr noCrossVal RidgeClassifierCV {'alphas': [0.1, 1.0, 10.0], 'fit_intercept': False, 'scoring': 'f1_macro', 'cv': 10, 'class_weight': None, 'store_cv_values': True} time: 1684765266.1070561 
errrr: cv!=None and store_cv_values=True are incompatible 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2543, in fit
    super().fit(X, target, sample_weight=sample_weight)
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2168, in fit
    raise ValueError("cv!=None and store_cv_values=True are incompatible")
ValueError: cv!=None and store_cv_values=True are incompatible
 
 
fitModelAndGetResults Errrrrrr noCrossVal RidgeClassifierCV {'alphas': [0.1, 1.0, 10.0], 'fit_intercept': False, 'scoring': 'f1_macro', 'cv': 10, 'class_weight': 'balanced', 'store_cv_values': True} time: 1684765266.1190424 
errrr: cv!=None and store_cv_values=True are incompatible 
traceback: Traceback (most recent call last):
  File "f:\projects\public github projects\private repos\binary-classification-telco-churn\modelEvaluatorModule.py", line 108, in fitModelAndGetResults
    fitted_model = model.fit(foldXTrain, foldYTrain.ravel())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2543, in fit
    super().fit(X, target, sample_weight=sample_weight)
  File "C:\Users\USER GOOD\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_ridge.py", line 2168, in fit
    raise ValueError("cv!=None and store_cv_values=True are incompatible")
ValueError: cv!=None and store_cv_values=True are incompatible
 
 
